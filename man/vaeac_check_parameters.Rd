% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/approach_vaeac_extra_functions.R
\name{vaeac_check_parameters}
\alias{vaeac_check_parameters}
\title{Function that calls all vaeac parameters check functions}
\usage{
vaeac_check_parameters(
  x_train,
  model_description,
  folder_to_save_model,
  cuda,
  num_vaeacs_initiate,
  epochs_initiation_phase,
  epochs,
  epochs_early_stopping,
  save_every_nth_epoch,
  validation_ratio,
  validation_iwae_num_samples,
  depth,
  width,
  latent_dim,
  lr,
  batch_size,
  running_avg_num_values,
  activation_function,
  skip_connection_layer,
  skip_connection_masked_enc_dec,
  batch_normalization,
  paired_sampling,
  masking_ratio,
  mask_gen_these_coalitions,
  mask_gen_these_coalitions_prob,
  sigma_mu,
  sigma_sigma,
  save_data,
  log_exp_cont_feat,
  which_vaeac_model,
  verbose,
  seed,
  ...
)
}
\arguments{
\item{x_train}{A data.table containing the data. Categorical names
Categorical data must have class names \eqn{1,2,\dots,K}.}

\item{model_description}{String containing, e.g., the name of the data distribution or
additional parameter information. Used in the save name of the fitted model.}

\item{folder_to_save_model}{String specifying a path to a folder where
the function is to save the fitted vaeac model.}

\item{cuda}{Boolean. If we are to use cuda (GPU) if available. STILL IN DEVELOPMENT!}

\item{num_vaeacs_initiate}{Integer. The number of different vaeac models to initiate in the start.
Pick the best performing one after \code{epochs_initiation_phase} and continue training that one.}

\item{epochs_initiation_phase}{Integer. The number of epochs to run each of the \code{num_vaeacs_initiate}
vaeac models before only continuing training the best one.}

\item{epochs}{Integer. The number of epochs to train the final vaeac model.
This includes \code{epochs_initiation_phase}.}

\item{epochs_early_stopping}{Integer. The training stops if there has been no improvement in the validation IWAE
for \code{epochs_early_stopping} epochs. If the user wants the training process to be solely based on this, then \code{epochs}
should be set to a large number.}

\item{save_every_nth_epoch}{Integer. If we are to save the vaeac model after every nth epoch.}

\item{validation_ratio}{Scalar between 0 and 1 indicating the ratio of
instances from data which will be used as validation data.}

\item{validation_iwae_num_samples}{Integer. The number of samples used to compute the
IWAE when validating the vaeac model on the validation data.}

\item{depth}{Integer. The number of hidden layers in the neural
networks of the masked encoder, full encoder, and decoder.}

\item{width}{Integer. The number of neurons in each hidden layer in
the neural networks of the masked encoder, full encoder, and decoder.}

\item{latent_dim}{Integer. The number of dimensions in the latent space.}

\item{lr}{Numeric. The learning rate used in the \code{\link[torch:optim_adam]{torch::optim_adam()}} optimizer.}

\item{batch_size}{Integer. The number of samples to include in each batch.}

\item{running_avg_num_values}{Integer. How many of the previous values to include when we compute the running means.}

\item{activation_function}{An \code{\link[torch:nn_module]{torch::nn_module()}} representing an activation function such as, e.g.,
\code{\link[torch:nn_relu]{torch::nn_relu()}}, \code{\link[torch:nn_leaky_relu]{torch::nn_leaky_relu()}}, \code{\link[torch:nn_selu]{torch::nn_selu()}}, and
\code{\link[torch:nn_sigmoid]{torch::nn_sigmoid()}}.}

\item{skip_connection_layer}{Boolean. If we are to use skip connections in each layer. If true, then we add the input
to the outcome of each hidden layer, so the output becomes X + activation(WX + b). I.e., identity skip connection.}

\item{skip_connection_masked_enc_dec}{Boolean. If we are to apply concatenate skip
connections between the layers in the masked encoder and decoder.}

\item{batch_normalization}{Boolean. If we are to use batch normalization after the activation function.
Note that if \code{skip_connection_layer} is TRUE, then the normalization is
done after the adding from the skip connection. I.e, we batch normalize the whole quantity X + activation(WX + b).}

\item{paired_sampling}{Boolean. Default is \code{TRUE}. If we are doing paired sampling. I.e.,
masked by \eqn{S} and the second one is masked by \eqn{\bar{S}}, the complement, see
\href{https://arxiv.org/pdf/2107.07436.pdf}{Jethani et al. (2022)}. Training becomes more
stable, but slower due to more complex implementation.}

\item{masking_ratio}{Probability of masking a feature in the MCAR mask generator.
Default masking scheme which ensures that vaeac can do arbitrary conditioning.
This is overruled if \code{mask_gen_these_coalitions} is specified.}

\item{mask_gen_these_coalitions}{Matrix containing the different coalitions to learn.}

\item{mask_gen_these_coalitions_prob}{Numerics containing the probabilities for
sampling each mask in \code{mask_gen_these_coalitions}.
Array containing the probabilities for sampling the coalitions in \code{mask_gen_these_coalitions}.}

\item{sigma_mu}{Numeric representing a hyperparameter in the normal-gamma prior used on the masked encoder,
see Section 3.3.1 in \href{https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf}{Olsen et al. (2022)}.}

\item{sigma_sigma}{Numeric representing a hyperparameter in the normal-gamma prior used on the masked encoder,
see Section 3.3.1 in \href{https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf}{Olsen et al. (2022)}.}

\item{save_data}{Boolean. If we are to save the data together with the model. Useful if one are to continue
to train the model later.}

\item{log_exp_cont_feat}{Boolean. If we are to log transform all continuous features before
sending the data to the vaeac using the \code{\link[=vaeac_postprocess_data]{vaeac_postprocess_data()}} function. The vaeac method creates
unbounded values, so if the continuous features are strictly positive, as for Burr and Abalone data, it can be
advantageous to log-transform the data to unbounded form before using vaeac.
If \code{TRUE}, then \code{\link[=vaeac_postprocess_data]{vaeac_postprocess_data()}} will take the exp of the results
to get back to strictly positive values when using the vaeac model to impute missing values.}

\item{verbose}{Boolean. If we are to print the progress of the initialization of different vaeac models,
the training of the final vaeac model, and summary of the training progress.}

\item{seed}{Positive integer (default is \code{1}). Seed for reproducibility.}

\item{...}{List of extra parameters, currently not used.}
}
\value{
The function does not return anything.
}
\description{
Function that calls all vaeac parameters check functions
}
\author{
Lars Henry Berge Olsen
}
\keyword{internal}
