---
title: "Shapley value explanations using the regression paradigm"
author: "Lars Henry Berge Olsen"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Shapley value explanations using the regression paradigm}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  markdown:
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.cap = "",
  fig.width = 7,
  fig.height = 5,
  fig.path = "figure_regression/", # Ensure that figures are saved in the right folder (this vignette will be built manually)
  cache.path = "cache_regression/", # Ensure that cached objects are saved in the right folder
  warning = FALSE,
  message = TRUE
)
```


> [The separate regression method class](#separate)

> [The surrogate regression method class](#surrogate)

> [Add new regression methods](#new)

> [Summary figures](#summary)

<a id="intro"></a>


In this vignette, we elaborate and illustrate the regression paradigm explained in @olsen2023comparative.
We describe how to specify the regression model, how to enable automatic cross-validation of the model's
hyper-parameters, and applying pre-processing steps to the data before fitting the regression models. 
We refer to @olsen2023comparative for when one should use the different paradigms, method classes, and methods.

The regression paradigm can be divided into the separate and surrogate regression method classes. In this vignette,
we give a short introduction to the two method classes. For an in depth explanation, we refer the reader to 
Sections 3.5 and 3.6 in @olsen2023comparative.

Briefly stated, the regression paradigm uses regression models to directly estimate the contribution function
$v(S) = E[f(\boldsymbol{x})|\boldsymbol{x}_S = \boldsymbol{x}_S^*]$. The separate regression method class
fits a separate regression model for each coalition $S$, while the surrogate regression method class fits
a single regression model to predict the contribution function for all coalitions simultaneously.

The `shapr` package supports any regression model from the popular *tidymodels* package, developed by @tidymodels.
See [list of tidymodels](https://www.tidymodels.org/find/parsnip/) for a complete list of the 
supported regression models. There are currently 80 supported models, but we can also a apply a wide range
of pre-processing data steps to increase this number. For example, the linear regression model can be applied directly
to the data or we can pre-process the data to compute principal components (principal component regression).

Our framework does not currently support model formulas with special terms. E.g., we do not support
`parsnip::gen_additive_mod` (i.e., `mgcv::gam()`) as it uses a non-standard notion in its formulas
(in this case the `s(feature, k = 2)` functions). See `?parsnip::model_formula()` for more information.
However, this hurdle can be overcome by using pre-processing data steps containing spline functions. 
We give an example of this in the [separate regression method class](#separate) section.

We can also [add new regression methods](#new) that are not already implemented in `tidymodel`. 
We demonstrate how to do this for the projection pursuit regression model.

Note that we use the same data and predictive models in this vignette as in the main vignette.

See the end of the vignette for summary figures of all the methods used in this vignette.


# The separate regression method class {#separate}
In the \separatereg\ methods, we train a new regression model $g_S(\boldsymbol{x}s)$ to estimate
the conditional expectation for each coalition of features. 

The idea is to estimate $v(S) = E[f(\boldsymbol{x})|\boldsymbol{x}_S = \boldsymbol{x}_S^*] = E[f(\boldsymbol{x}_{\bar{S}},\boldsymbol{x}_S)|\boldsymbol{x}_S=\boldsymbol{x}_S^*]$ separately
for each coalition $S$ using regression. Let $X = \{ \boldsymbol{x}^{[i]}, y^{[i]} \}_{i=1}^{N_{\text{train}}}$
denote the training data, where $\boldsymbol{x}^{[i]}$ is the $i$th $M$-dimensional input and $y^{[i]}$
is the associated response. For each coalition $S \subseteq \{1,2,\dots,M\}$, the corresponding training data set is
\begin{align*}
            X_S 
            =
            \{\boldsymbol{x}_S^{[i]}, f(\underbrace{\boldsymbol{x}_\bar{S}^{[i]}, \boldsymbol{x}_S^{[i]}}_{\boldsymbol{x}^{[i]}})\}_{i=1}^{N_{\text{train}}}
            =
            \{\boldsymbol{x}_S^{[i]}, \underbrace{f(\boldsymbol{x}^{[i]})}_{z^{[i]}}\}_{i=1}^{N_{\text{train}}}
            =
            \{\boldsymbol{x}_S^{[i]}, z^{[i]}\}_{i=1}^{N_{\text{train}}}.
\end{align*}

For each data set $X_S$, we train a regression model $g_S(\boldsymbol{x}s)$ with respect to the mean squared error loss
function. The optimal model, with respect to the loss function, is 
$g^*_S(\boldsymbol{x}_S) = E[z|\boldsymbol{x}_S] = E[f(\boldsymbol{x}_\bar{S}, \boldsymbol{x}_S)|\boldsymbol{x}_S]$,
which corresponds to the contribution function $v(S)$. The regression model $g_S$ aims for the optimal, hence, it
resembles/estimates the contribution function, i.e., 
$g_S(\boldsymbol{x}_S) = \hat{v}(S) \approx v(S) = E[f(\boldsymbol{x}_\bar{S}, \boldsymbol{x}_S) | \boldsymbol{x}_S = \boldsymbol{x}_S^*]$. 



## Code {#separate_code}
In the following examples we use the same data and want to explain the same model as in the main vignette.
That is, we train a simple `xgboost` model on the `airquality` dataset, and illustrate how `shapr` and 
the separate regression method class can be used to explain the individual predictions.

First we setup the `airquality` dataset and train the `xgboost` model.
```{r setup, message=FALSE}
library(xgboost)
library(tidymodels)
library(data.table)
library(shapr)

data("airquality")
data <- data.table::as.data.table(airquality)
data <- data[complete.cases(data), ]

x_var <- c("Solar.R", "Wind", "Temp", "Month")
y_var <- "Ozone"

ind_x_explain <- 1:20
x_train <- data[-ind_x_explain, ..x_var]
y_train <- data[-ind_x_explain, get(y_var)]
x_explain <- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model <- xgboost::xgboost(
  data = as.matrix(x_train),
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Specifying the phi_0, i.e. the expected prediction without any features
p0 <- mean(y_train)

# List to store all the explanation objects
explanation_list <- list()

# Create helper function to plot the MSEv criterion scores
plot_MSEv_scores <- function(explanation_list, method_line = NULL) {
  fig <- plot_MSEv_eval_crit(explanation_list) +
    ggplot2::theme(legend.position = "none") +
    ggplot2::coord_flip() +
    ggplot2::theme(plot.title = ggplot2::element_text(size = rel(0.95)))
  fig <- fig + ggplot2::scale_x_discrete(limits = rev(levels(fig$data$Method)))
  if (!is.null(method_line) && method_line %in% fig$data$Method) {
    fig <- fig + ggplot2::geom_hline(
      yintercept = fig$data$MSEv[fig$data$Method == method_line],
      linetype = "dashed",
      color = "black"
    )
  }
  return(fig)
}

# Create helper function to print out the MSEv criterion scores
print_MSEv_scores_and_time <- function(explanation_list) {
  res <- t(sapply(
    explanation_list,
    function(explanation) {
      c(explanation$MSEv$MSEv$MSEv, round(explanation$timing$total_time_secs, 2))
    }
  ))
  colnames(res) <- c("MSEv", "Time")
  return(res)
}
```

To have a baseline to compare the regression methods with, we will in this vignette compare them with
the Monte Carlos based `empirical` approach with default hyperparameters. In the last section, we include
all Monte Carlo based methods implemented in `shapr`, and it is `empirical` that performs the best.
```{r empirical, cache=TRUE}
# Compute the Shapley value explanations using the empirical method
explanation_list$MC_empirical <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "empirical",
  prediction_zero = p0,
    n_batches = 4
)
```




### Linear regression model
Then we compute the Shapley value explanations using a linear
regression model and the separate regression method class.
```{r explain-sep-lm, cache=TRUE}
explanation_list$sep_lm <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
    n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::linear_reg()
)
```

A linear model is often not flexible enough to properly model the contribution function. Thus, it can produce
inaccurate Shapley value explanations. We see in the figure below that the `empirical` approach outperforms the
linear regression model approach quite significantly with respect to the $\operatorname{MSE}_v$ evaluation criterion.

```{r lm-emp-msev, cache=TRUE}
plot_MSEv_scores(explanation_list)
```

### Pre-processing
In this section, we describe how to pre-process the data before fitting the separate regression models.
We illustrate this for the linear regression model, but this pre-processing can be applied to other regression 
methods too. 

The *recipe* package in `tidymodels` contains a wide range of functions for allowing us to preprocess the data before 
fitting the model. This can be, for example, applying transformations (log, poly, splines, pls, pca), encodings,
interactions, normalization, and so on. 
See [list of functions](https://recipes.tidymodels.org/reference/index.html) for all list of all possible functions.
The list also contains functions for helping us selecting which features to apply the functions too, e.g., 
`recipes::all_predictors()`, `recipes::all_numeric_predictors()`, and `recipes::all_factor_predictors()` apply the
functions to all features, only the numerical features, and only the factor features, respectively. You can also
specify the names of the features to apply the functions too. However, as the included features changes in each
coalition, we need to do a check that the feature we want to apply the function to is present in the dataset.
We give an example of this below.


First, we demonstrate how to compute the principal components and use (up-to) the first two components for
each separate linear regression models. We write up-to as for the singleton coalitions, we can only compute a single
component (the feature in itself). This is called principal component regression. 
```{r pcr, cache=TRUE}
explanation_list$sep_pcr <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
    n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::linear_reg(),
  regression_recipe_func = function(regression_recipe) {
    return(step_pca(regression_recipe, all_numeric_predictors(), num_comp = 2))
  }
)
```

Second, we use natural spline basis functions with two degrees of freedom. This is similar to fitting a GAM.
```{r natural-splines, cache=TRUE}
explanation_list$sep_splines <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
    n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::linear_reg(),
  regression_recipe_func = function(regression_recipe) {
    return(step_ns(regression_recipe, all_numeric_predictors(), deg_free = 2))
  }
)
```

Finally, we provide an example where we include interactions between the features `Solar.R` and `Wind`,
log-transform `Solar.R`, convert `Wind` to be between 0 and 1 and then take the square root, 
include polynomials of the third degree for `Temp`, and apply the Box Cox transformation to `Month`. 
These transformations are only applied when the features are present for the different separate models.

Furthermore, we stress that the purpose of this example is to highlight the flexibility of the framework and
NOT that the transformations below are reasonable to do. 
```{r recipe-func, cache=TRUE}
# Example function of how to apply step functions from the recipes package to specific features
regression_recipe_func <- function(regression_recipe) {
  # Get the names of the present features
  feature_names <- regression_recipe$var_info$variable[regression_recipe$var_info$role == "predictor"]

  # If Solar.R and Wind is present, then we add the interaction between them
  if (all(c("Solar.R", "Wind") %in% feature_names)) {
    regression_recipe <- step_interact(regression_recipe, terms = ~ Solar.R:Wind)
  }

  # If Solar.R is present, then log transform it
  if ("Solar.R" %in% feature_names) regression_recipe <- step_log(regression_recipe, Solar.R)

  # If Wind is present, then scale it to be between 0 and 1 and then sqrt transform it
  if ("Wind" %in% feature_names) regression_recipe <- step_sqrt(step_range(regression_recipe, Wind))

  # If Temp is present, then expand it using orthogonal polynomials of degree 3
  if ("Temp" %in% feature_names) regression_recipe <- step_poly(regression_recipe, Temp, degree = 3)

  # If Month is present, then Box-Cox transform it
  if ("Month" %in% feature_names) regression_recipe <- step_BoxCox(regression_recipe, Month)

  # Finally we normalize all features (not needed as LM does this internally)
  regression_recipe <- step_normalize(regression_recipe, all_numeric_predictors())

  return(regression_recipe)
}

# Compute the Shapley values using the pre-processing steps defined above
explanation_list$sep_reicpe_example <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
    n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::linear_reg(),
  regression_recipe_func = regression_recipe_func
)
```

```{r preproc-plot, cache=TRUE}
# Compare the MSEv criterion of the different explanation methods
plot_MSEv_scores(explanation_list)

# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)
```




### Other regression models

In the next example, we use a decision tree model instead of the simple linear regression model.

The `tidymodels` package supports several implementations for the decision tree model. 
Thus, we have to use `set_engine("rpart")` to specify that we want to use the implementation
in the `rpart` package  and we need to specify that we are going to do regression by using 
`set_mode("regression")`. Note that we do not specify the parameters of the decision tree,
which means that `tidymodels` will use the default parameters set in `rpart`. 

By searching for "decision tree" in [list of tidymodels](https://www.tidymodels.org/find/parsnip/),
we will see that the default parameters for
[`decision_tree_rpart`](https://parsnip.tidymodels.org//reference/details_decision_tree_rpart.html)
are `tree_depth = 30`, `min_n = 2`, and `cost_complexity = 0.01`.

```{r decision-tree, cache=TRUE}
# Decision tree with specified parameters (stumps)
explanation_list$sep_tree_stump <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::decision_tree(tree_depth = 1, min_n = 2, cost_complexity = 0.01) %>%
    parsnip::set_engine("rpart") %>%
    parsnip::set_mode("regression")
)

# Decision tree with default parameters
explanation_list$sep_tree_default <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model =
    parsnip::decision_tree() %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression")
)
```
We can also set `regression_model = parsnip::decision_tree(mode = "regression", engine = "rpart", tree_depth = 1, min_n = 2, cost_complexity = 0.01)` if we do not want to use the pipe function (`%>%`).

We can now compare the two new methods. We see that the decision tree with default parameters outperform 
the linear model approach with respect to the $\operatorname{MSE}_v$ criterion, and is on the same level
as the empirical approach. By using stumps, i.e., trees with depth one, we obtained a worse method.

```{r decision-tree-plot, cache=TRUE}
# Compare the MSEv criterion of the different explanation methods
plot_MSEv_scores(explanation_list)

# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)
```

### Cross validation

Another option is to use cross validation to tune the hyperparameters. To do this, we need to specify three things:

1. In `regression_model`, we need to specify which parameters to tune in the model. 
This is done by setting the parameter equal to `tune()`. E.g., if we want to
tune the `tree_depth` in the `decision_tree`, while using default parameters for the
other parameters, then we set `decision_tree(tree_depth = tune())`.
2. In `regression_tune_values`, we must provide either a data.frame/data.table/tibble of the
possible hyperparameter values to consider or a function that takes in the training data for 
each combination/coalition and outputs the a data.frame with the possible hyperparameter values.
The latter allows us to use different hyperparameter values for the different coalition sizes.
This is also essential if the domain of a hyperparameter changes with the coalition size. E.g., 
`mtry` in `ranger` (random forest). See example below. 
The column names of `regression_tune_values` (or the output if it is a function)
must match the tuneable hyperparameters specified in `regression_model`. I.e., for the
example above, `regression_tune_values` must be a one column data.frame/data.table/tibble
with the column name `tree_depth`. This can either be done by specifying the values manually
or using the `dials` package, e.g., `dials::grid_regular(dials::tree_depth(), levels = 5)`. 
Or it can be a function that outputs a data.frame on the same form.
3. It is optional to specify the `regression_vfold_cv_para` parameter. If used, then
`regression_vfold_cv_para` must be a named list specifying the parameters to send to the
cross-validation function `rsample::vfold_cv()`. See `?rsample::vfold_cv` to see the default
parameters. The names in `regression_vfold_cv_para` must match the parameter names in
`rsample::vfold_cv()`. For example, if we want 5-fold cross validation, then we set
`regression_vfold_cv_para = list(v = 5)`.

First, let us look at some ways to specify `regression_tune_values`. 
Note that `dials` have several other grid functions too, e.g., `dials::grid_random()` 
and `dials::grid_latin_hypercube()`.
```{r echo=TRUE, results='hide'}
# Possible ways to define the `regression_tune_values` object. One automatic and three manual ways.
# function(x) return(dials::grid_regular(dials::tree_depth(), levels = 4))
dials::grid_regular(dials::tree_depth(), levels = 4)
data.table(tree_depth = c(1, 5, 10, 15)) # Note that we can also use data.frame and tibble too.

# For several features
# function(x) return(dials::grid_regular(dials::tree_depth(), dials::cost_complexity(), levels = 3))
dials::grid_regular(dials::tree_depth(), dials::cost_complexity(), levels = 3)
expand.grid(tree_depth = c(1, 3, 5), cost_complexity = c(0.001, 0.05, 0.01))
```

We are now ready to to use cross validation to fine tune the separate decision tree regression method. In the 
following examples, we consider to versions. In the first example, we use cross validation on the `tree_depth` parameter
using the `dials::grid_regular()` function. In the second example, we tune the `tree_dept()` and `cost_complexity()`
parameters, where the possible hyperparameter values have been manually specified.

For the first version, we set `verbose = 2` to see the printouts. We get messages about which batch and
coalition/combination that are currently being processed and the results of the cross-validation procedure.
We skip the prinout for the second version due to the length of the printouts.

```{r dt-cv, cache=TRUE}
# Decision tree with cross validated depth (default values other parameters)
explanation_list$sep_tree_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  verbose = 2, # If we want a prinout of the results
  approach = "regression_separate",
  regression_model =
    parsnip::decision_tree(tree_depth = tune()) %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression"),
  regression_tune_values = dials::grid_regular(dials::tree_depth(), levels = 4),
  regression_vfold_cv_para = list(v = 5)
)

# Use trees with cross validation on the depth and cost complexity. Manually set the values.
explanation_list$sep_tree_cv_2 <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model =
    parsnip::decision_tree(tree_depth = tune(), cost_complexity = tune()) %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression"),
  regression_tune_values =
    expand.grid(tree_depth = c(1, 3, 5), cost_complexity = c(0.001, 0.01, 0.1)),
  regression_vfold_cv_para = list(v = 5)
)
```
We also include one example with a random forest model where the tunable hyperparameter `mtry` is dependent 
on the coalition size, hence, we let `regression_tune_values` be a function. If we do not let `regression_tune_values`
be a function, then `tidymodels` will crash for any `mtry` larger than 1. We also set `verbose = 2` to illustrate
that the tested hyperparameter value combinations changes best on the coalition size.
```{r}
# Using random forest with default parameters
explanation_list$sep_rf <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::rand_forest() %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression")
)

# Using random forest with parameters tuned by cross validation
explanation_list$sep_rf_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 1, # One batch to get printouts in chronological order
  verbose = 2,
  approach = "regression_separate",
  regression_model = parsnip::rand_forest(mtry = tune(), trees = tune()) %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = 
    function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x))), dials::trees(c(50, 750)), levels = 3)),
  regression_vfold_cv_para = list(v = 5)
)
```


We can look at the $\operatorname{MSE}_v$ evaluation criterion and we see that conducting cross-validation
drastically improves both the decision tree method and the random forest method. 
The two cross-validated decision tree methods are very comparable, but the second version
outperforms the first version with a small margin. Note that this a bit of an unfair comparison for the `empirical`
approach which also has hyper-parameters one could tune. However, `shapr` does not currently provide a function
to do this automatically. In the figure below, we include a vertical line at the $\operatorname{MSE}_v$ score
of the `empirical` method.

```{r dt-cv-plot, cache=TRUE}
plot_MSEv_scores(explanation_list, method_line = "MC_empirical") 
```

Furthermore, we have to consider that doing the cross validation drastically increase the elapsed time in seconds.
Thus, we have to evaluate if the better precision is worth the extra elapsed time. We also see that the complex
random forest method performs significantly worse than the simple decision tree method. This can indicate that
even though we do hyperparameter tuning, we are still overfitting the data.

```{r dt-cv-print, cache=TRUE}
# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)
```


### Parallelization
The separate regression models can be trained in parallel using the *future* package. This is easily done with the
`shapr` package as explained in the main vignette, where also how to enabling progress bars are discussed. 

We use three methods here. The first method use default `xgboost` models, while the other two use cross-validation
to tune the number of trees. The latter two specify the same potential hyperparameter values, but the first one is
run sequentially while the second one is run in parallel to speed up the computations.

```{r xgboost, cache=TRUE}
# Regular xgboost with default parameters
explanation_list$sep_xgboost <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::boost_tree() %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression")
)

# Do cross validation on number of trees
explanation_list$sep_xgboost_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::boost_tree(trees = tune()) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = expand.grid(trees = c(5, 15, 25)),
  regression_vfold_cv_para = list(v = 5)
)

# Do cross validation on number of trees but this time in parallel on two threads
future::plan(future::multisession, workers = 2)
explanation_list$sep_xgboost_cv_par <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::boost_tree(trees = tune()) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = expand.grid(trees = c(5, 15, 25)),
  regression_vfold_cv_para = list(v = 5)
)

explanation_list$sep_xgboost_cv_2_par <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::boost_tree(trees = tune(), tree_depth = tune()) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = expand.grid(trees = c(5, 15, 25), tree_depth = c(2, 6, 10)),
  regression_vfold_cv_para = list(v = 5)
)
future::plan(future::sequential) # To return to non-parallel computation
```

We can look at the elapsed time and we see that the parallel version is twice as fast as the sequential version.
Furthermore, we see that conducting the cross-validation has lowered the $\operatorname{MSE}_v$ criterion drastically.
Also note that we obtain the same value no matter if we run the cross-validation in parallel or sequentially.
```{r xgboost-print, cache=TRUE}
# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)
```






# The surrogate regression method class {#surrogate}
Since the \separatereg\ methods train a new regression model $g_S(\boldsymbol{x}_S)$ for each coalition
$S \subseteq \{1,2,\dots,M\}$, a total of $2^M-2$ models has to be trained, which can be time-consuming for
slowly fitted models. The minus two corresponds to the empty and grand coalitions.

The \surrogatereg\ method class builds on the ideas from the \separatereg\ class, but instead of fitting a
new regression model for each coalition, we train a single regression model $g(\tilde{\boldsymbol{x}}_S)$
for all coalitions $S \subseteq \{1,2,\dots,M\}$ (except empty and grand coalitions),
where $\tilde{\boldsymbol{x}}_S$ is an augmented version of $\boldsymbol{x}_S$. See Section 3.6.1 in
@olsen2023comparative for more details and examples.

All the examples given above for the separate regression method class can also be applied for the
surrogate regression method class.


## Code {#surrogate_code}

We illustrate the surrogate method class with several regression models. More specifically, we use linear regression,
random forest (with and without (some) cross-validation), and xgboost (with and without (some) cross-validation).

```{r surrogate, cache=TRUE}
# Compute the Shapley value explanations using a surrogate linear regression model
explanation_list$sur_lm <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::linear_reg()
)

# Using random forest with default parameters as the surrogate model
explanation_list$sur_rf <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::rand_forest() %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression")
)

# Using random forest with parameters tuned by cross validation as the surrogate model
explanation_list$sur_rf_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::rand_forest(mtry = tune(), trees = tune()) %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression"),
  regression_tune_values =
    dials::grid_regular(dials::mtry(c(1, ncol(x_explain))), dials::trees(c(50, 750)), levels = 4),
  regression_vfold_cv_para = list(v = 5)
)

# Using xgboost with default parameters as the surrogate model
explanation_list$sur_xgboost <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::boost_tree() %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression")
)

# Using xgboost with default parameters as the surrogate model
explanation_list$sur_xgboost_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::boost_tree(trees = tune(), tree_depth = tune()) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = expand.grid(trees = c(5, 15, 25), tree_depth = c(2, 6, 10)),
  regression_vfold_cv_para = list(v = 5)
)
```

We can then look at the 
```{r surrogate-plot, cache=TRUE}
# Compare the MSEv criterion of the different explanation methods.
# Include vertical line corresponding to the MSEv of the empirical method.
plot_MSEv_scores(explanation_list, method_line = "MC_empirical")

# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)
```


# Add new regression methods {#new}

Even though the `tidymodels` package contain a lot of models 
(See [list of tidymodels](https://www.tidymodels.org/find/parsnip/)), we might want to add additional methods.
In the following section, we illustrate how to add a the projection pursuit regression (PPR) model as 
new method that can be used by `shapr` to compute the Shapley value explanations;
both as a separate and surrogate method.

We use the `ppr()` implementation in the `stats` package to fit the PPR model. The model has several
hyperparameters that can be tuned, but the main hyperparameter is the number of terms `nterms`.
The following is based on [tidymodels' guide](https://www.tidymodels.org/learn/develop/models/) and
we refer to that guide for more details and explanations of the code below.


```{r ppr-setup, cache=TRUE}
# Step 1: register the model, modes, and arguments
parsnip::set_new_model(model = "ppr_reg")
parsnip::set_model_mode(model = "ppr_reg", mode = "regression")
parsnip::set_model_engine(model = "ppr_reg", mode = "regression", eng = "ppr")
parsnip::set_dependency("ppr_reg", eng = "ppr", pkg = "stats")

# If your function has several parameters, then we add one of these functions for each parameter
parsnip::set_model_arg(
  model = "ppr_reg",
  eng = "ppr",
  original = "nterms", # The original parameter name used in stats::ppr
  parsnip = "num_terms", # Change parameter name to match tidymodels' name convention
  func = list(pkg = "dials", fun = "num_terms"), # list(pkg = "stats", fun = "ppr"),
  has_submodel = FALSE
)

# Step 2: create the model function
ppr_reg <- function(mode = "regression", num_terms = NULL) {
  # Check for correct mode
  if (mode != "regression") rlang::abort("`mode` should be 'regression'")

  # Capture the arguments in quosures
  args <- list(num_terms = rlang::enquo(num_terms))

  # Save some empty slots for future parts of the specification
  parsnip::new_model_spec(
    "ppr_reg",
    args = args,
    eng_args = NULL,
    mode = mode,
    method = NULL,
    engine = NULL
  )
}

# Step 3: add a fit module
parsnip::set_fit(
  model = "ppr_reg",
  eng = "ppr",
  mode = "regression",
  value = list(
    interface = "formula",
    protect = c("formula", "data", "weights"),
    func = c(pkg = "stats", fun = "ppr"),
    defaults = list()
  )
)

parsnip::set_encoding(
  model = "ppr_reg",
  eng = "ppr",
  mode = "regression",
  options = list(
    predictor_indicators = "traditional",
    compute_intercept = TRUE,
    remove_intercept = TRUE,
    allow_sparse_x = FALSE
  )
)

# Step 4: add modules for prediction
parsnip::set_pred(
  model = "ppr_reg",
  eng = "ppr",
  mode = "regression",
  type = "numeric",
  value = list(
    pre = NULL,
    post = NULL,
    func = c(fun = "predict"),
    args = list(
      object = quote(object$fit),
      newdata = quote(new_data),
      type = "numeric"
    )
  )
)

# Step 5: add tuning function (used by tune::tune_grid())
tunable.ppr_reg <- function(x, ...) {
  tibble::tibble(
    name = c("num_terms"),
    call_info = list(list(pkg = NULL, fun = "num_terms")),
    source = "model_spec",
    component = "ppr_reg",
    component_id = "main"
  )
}

# Step 6: add updating function (used by tune::finalize_workflow())
update.ppr_reg <- function(object, parameters = NULL, num_terms = NULL, ...) {
  rlang::check_installed("parsnip")
  eng_args <- parsnip::update_engine_parameters(object$eng_args, fresh = TRUE, ...)
  args <- list(num_terms = rlang::enquo(num_terms))
  args <- parsnip::update_main_parameters(args, parameters)
  parsnip::new_model_spec(
    "ppr_reg",
    args = args,
    eng_args = eng_args,
    mode = object$mode,
    method = NULL,
    engine = object$engine
  )
}
```


We can now use the PPR model to compute the Shapley value explanations. We can use it as both a separate and
surrogate approach, and we can either set the number of terms `num_terms` to a specific value or we can use cross-validation to tune it. We do all four combinations below.

```{r ppr-train, cache=TRUE}
# PPR separate with specified number of terms
explanation_list$sep_ppr <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = ppr_reg(num_terms = 2) %>% set_engine("ppr") %>% set_mode("regression")
)

# PPR separate with cross-validated number of terms
explanation_list$sep_ppr_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = ppr_reg(num_terms = tune()) %>%
    parsnip::set_engine("ppr") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = dials::grid_regular(dials::num_terms(c(1, 4)), levels = 3),
  regression_vfold_cv_para = list(v = 10)
)

# PPR surrogate with specified number of terms
explanation_list$sur_ppr <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = ppr_reg(num_terms = 3) %>%
    parsnip::set_engine("ppr") %>%
    parsnip::set_mode("regression")
)

# PPR surrogate with cross-validated number of terms
explanation_list$sur_ppr_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = ppr_reg(num_terms = tune()) %>%
    parsnip::set_engine("ppr") %>%
    parsnip::set_mode("regression"),
  regression_tune_values = dials::grid_regular(dials::num_terms(c(1, 8)), levels = 4),
  regression_vfold_cv_para = list(v = 10)
)
```
We can then compare the $\operatorname{MSE}_v$ and some of the Shapley value explanations.
We see that conducting cross-validation improves the evaluation criterion,
but also increase the running time.

```{r ppr-plot, cache=TRUE}
# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)

# Compare the MSEv criterion of the different explanation methods
plot_MSEv_scores(explanation_list)
```


# Summary figures {#summary}
Here we include all the approaches that we 
```{r MC, cache=TRUE}
explanation_list_MC <- list()

# Compute the Shapley value explanations using the empirical method
explanation_list_MC$MC_independence <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  n_batches = 4,
  approach = "independence",
  prediction_zero = p0
)

explanation_list_MC$MC_empirical <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  n_batches = 4,
  approach = "empirical",
  prediction_zero = p0
)

explanation_list_MC$MC_gaussian <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  n_batches = 4,
  approach = "gaussian",
  prediction_zero = p0
)

explanation_list_MC$MC_copula <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  n_batches = 4,
  approach = "copula",
  prediction_zero = p0
)

explanation_list_MC$MC_ctree <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  n_batches = 4,
  approach = "ctree",
  prediction_zero = p0
)

explanation_list_MC$MC_vaeac <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  n_batches = 4,
  approach = "vaeac",
  prediction_zero = p0,
  vaeac.epochs = 10
)

# Combine the two explanations lists
explanation_list[1] <- NULL
explanation_list <- c(explanation_list_MC, explanation_list)
```


We can now look at plots of the $\operatorname{MSE}_v$ evaluation criterion. We include a 
vertical line corresponding to the $\operatorname{MSE}_v$ of the `empirical` method to make it easier to compare the
regression methods with the Monte Carlo based methods. 

```{r MSEv-sum, cache=TRUE}
# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list)

# Compare the MSEv criterion of the different explanation methods
# Include vertical line corresponding to the MSEv of the empirical method.
plot_MSEv_scores(explanation_list, method_line = "MC_empirical")
```

We can also look at how the different Shapley value explanations looks for the first six explicands (two at the time). 
We see that most methods agree in the general directions, especially for the most important features 
(the features with largest Shapley values in absolute value), but there are some differences for the less
important features.
```{r SV-sum, cache=TRUE}
plot_SV_several_approaches(explanation_list, index_explicands = c(1, 2), facet_ncol = 1)
plot_SV_several_approaches(explanation_list, index_explicands = c(3, 4), facet_ncol = 1)
plot_SV_several_approaches(explanation_list, index_explicands = c(5, 6), facet_ncol = 1)
```


Sort them and only show for some






# Mixed data {#mixed}

In this section, we replicate and extend the mixed data example from the main vignette by demonstrating
how to use the separate and surrogate regression methods. Of the Monte Carlo based methods, only 
`independence` (not recommended), `ctree` and `vaeac` support mixed data. For the regression methods,
we can divide the regression models into two groups. Those that can handle categorical features by default
and those models we need to apply pre-processing of the categorical features. By pre-processing, we mean
that we need to convert them into numerical values using for example dummy features. We demonstrate how
this is done below using the `regression_recipe_func` function. 

## Mixed data: setup

First, we copy the setup from the main vignette.

```{r mixed-setup, cache = TRUE}
# convert the month variable to a factor
data_cat = copy(data)[, Month_factor := as.factor(Month)]

data_train_cat <- data_cat[-ind_x_explain, ]
data_explain_cat <- data_cat[ind_x_explain, ]

x_var_cat <- c("Solar.R", "Wind", "Temp", "Month_factor")

x_train_cat <- data_train_cat[, ..x_var_cat]
x_explain_cat <- data_explain_cat[, ..x_var_cat]

p0_cat <- mean(y_train)

# Fitting an lm model here as xgboost does not handle categorical features directly
formula <- as.formula(paste0(y_var, " ~ ", paste0(x_var_cat, collapse = " + ")))
model_cat <- lm(formula, data_train_cat)

# We could also consider other models such as random forest which supports mixed data
# model_cat <- ranger(formula, data_train_cat)

# List to store the explanations for this mixed data setup
explanation_list_mixed <- list()
```


## Mixed data: Monte Carlo-based methods
Second, we compute the explanations using the Monte Carlo Based methods.

```{r mixed-MC, cache = TRUE}
explanation_list_mixed$MC_independence <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "independence"
)

explanation_list_mixed$MC_ctree <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "ctree"
)

explanation_list_mixed$MC_vaeac_50_4 <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "vaeac",
  verbose = 2,
  vaeac.epochs = 5,
  n_samples = 20
)

explanation_list_mixed$MC_vaeac_200 <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "vaeac",
  vaeac.epochs = 200
)
```


## Mixed data: separate regression methods
Third, we use separate regression to compute the Shapley value explanations.
```{r mixed-separate, cache = TRUE}
# Standard linear regression
explanation_list_mixed$sep_lm <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::linear_reg()
)

# Linear regression where we have added splines to the numerical features
explanation_list_mixed$sep_splines <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::linear_reg(),
  regression_recipe_func = function(regression_recipe) {
    return(step_ns(regression_recipe, all_numeric_predictors(), deg_free = 2))
  }
)

# Decision tree with default parameters
explanation_list_mixed$sep_tree <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model =
    parsnip::decision_tree() %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression")
)

# Use trees with cross validation on the depth and cost complexity. Manually set the values.
explanation_list_mixed$sep_tree_cv <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model =
    parsnip::decision_tree(tree_depth = tune(), cost_complexity = tune()) %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression"),
  regression_tune_values =
    expand.grid(tree_depth = c(1, 3, 5), cost_complexity = c(0.001, 0.01, 0.1)),
  regression_vfold_cv_para = list(v = 5)
)

# Random forest with default hyperparameters. Do NOT need to use dummy features.
explanation_list_mixed$sep_rf <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::rand_forest() %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression")
)

# Random forest with cross validated hyperparameters.
explanation_list_mixed$sep_rf_cv <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::rand_forest(mtry = tune(), trees = tune()) %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression"),
  regression_tune_values =
    function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x))), dials::trees(c(50, 750)), levels = 4)),
  regression_vfold_cv_para = list(v = 5)
)

# Xgboost with default hyperparameters, but we have to dummy encode the factors
explanation_list_mixed$sep_xgboost <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::boost_tree() %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_recipe_func = function(regression_recipe) {
    return(step_dummy(regression_recipe, all_factor_predictors()))
  }
)

# Xgboost with cross validated hyperparameters and we dummy encode the factors
explanation_list_mixed$sep_xgboost_cv <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_separate",
  regression_model = parsnip::boost_tree(trees = tune(), tree_depth = tune()) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_recipe_func = function(regression_recipe) {
    return(step_dummy(regression_recipe, all_factor_predictors()))
  },
  regression_tune_values = expand.grid(trees = c(5, 15, 25), tree_depth = c(2, 6, 10)),
  regression_vfold_cv_para = list(v = 5)
)
```


## Mixed data: surrogate regression methods
Fourth, we use separate regression to compute the Shapley value explanations.
```{r mixed-surrogate, cache = TRUE}
# Standard linear regression
explanation_list_mixed$sur_lm <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::linear_reg()
)

# Linear regression where we have added splines to the numerical features
explanation_list_mixed$sur_splines <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::linear_reg(),
  regression_recipe_func = function(regression_recipe) {
    return(step_ns(regression_recipe, all_numeric_predictors(), deg_free = 2))
  }
)

# Decision tree with default parameters
explanation_list_mixed$sur_tree <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model =
    parsnip::decision_tree() %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression")
)

# Use trees with cross validation on the depth and cost complexity. Manually set the values.
explanation_list_mixed$sur_tree_cv <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model =
    parsnip::decision_tree(tree_depth = tune(), cost_complexity = tune()) %>%
      parsnip::set_engine("rpart") %>%
      parsnip::set_mode("regression"),
  regression_tune_values =
    expand.grid(tree_depth = c(1, 3, 5), cost_complexity = c(0.001, 0.01, 0.1)),
  regression_vfold_cv_para = list(v = 5)
)

# Random forest with default hyperparameters. Do NOT need to use dummy features.
explanation_list_mixed$sur_rf <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::rand_forest() %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression")
)

# Random forest with cross validated hyperparameters.
explanation_list_mixed$sur_rf_cv <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::rand_forest(mtry = tune(), trees = tune()) %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode("regression"),
  regression_tune_values =
    function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x))), dials::trees(c(50, 750)), levels = 4)),
  regression_vfold_cv_para = list(v = 5)
)

# Xgboost with default hyperparameters, but we have to dummy encode the factors
explanation_list_mixed$sur_xgboost <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::boost_tree() %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_recipe_func = function(regression_recipe) {
    return(step_dummy(regression_recipe, all_factor_predictors()))
  }
)

# Xgboost with cross validated hyperparameters and we dummy encode the factors
explanation_list_mixed$sur_xgboost_cv <- explain(
  model = model_cat,
  x_explain = x_explain_cat,
  x_train = x_train_cat,
  prediction_zero = p0_cat,
  n_batches = 4,
  approach = "regression_surrogate",
  regression_model = parsnip::boost_tree(trees = tune(), tree_depth = tune()) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode("regression"),
  regression_recipe_func = function(regression_recipe) {
    return(step_dummy(regression_recipe, all_factor_predictors()))
  },
  regression_tune_values = expand.grid(trees = c(5, 15, 25), tree_depth = c(2, 6, 10)),
  regression_vfold_cv_para = list(v = 5)
)
```

## Mixed data: summary
Fifth, and finally, we compare the results. 
```{r mixed-plot, cache = TRUE}
# Print the MSEv scores and the elapsed time (in seconds) for the different methods
print_MSEv_scores_and_time(explanation_list_mixed)

# Compare the MSEv criterion of the different explanation methods
# Include vertical line corresponding to the MSEv of the empirical method.
plot_MSEv_scores(explanation_list_mixed)

plot_SV_several_approaches(explanation_list_mixed, index_explicands = c(1, 2), facet_ncol = 1)
```
