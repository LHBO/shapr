---
title: "Shapley value explanations using the regression paradigm "
author: "Lars Henry Berge Olsen"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{The `vaeac` approach in `shapr`}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  markdown:
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.cap = "",
  fig.width = 7,
  fig.height = 5,
  fig.path = "figure_regression/", # Ensure that figures are saved in the right folder (this vignette will be built manually)
  cache.path = "cache_regression/", # Ensure that cached objects are saved in the right folder
  warning = FALSE,
  message = TRUE
)
```


> [The separate regression method class](#separate)

> [The surrogate regression method class](#surrogate)

> [Add new regression methods](#new)

<a id="intro"></a>


In this vignette, we elaborate and illustrate the regression paradigm explained in @olsen2023comparative.
We describe how to specify the regression model, how to enable automatic cross-validation of the model's
hyper-parameters, and applying pre-processing steps to the data before fitting the regression models.

The regression paradigm can be divided into the separate and surrogate regression method classes. In this vignette,
we give a short introduction to the two method classes. For an in depth explanation, we refer the reader to 
Section 3.5 and 3.6 in @olsen2023comparative.

Briefly stated, the regression paradigm uses regression models to directly estimate the contribution function
$v(S) = E[f(\boldsymbol{x})|\boldsymbol{x}_S = \boldsymbol{x}_S^*]$. The separate regression method class
fits a separate regression model for each coalition $S$, while the surrogate regression method class fits
a single regression model to predict the contribution function for all coalitions simultaneously.

The `shapr` package supports any regression model from the popular *tidymodels* package, developed by @tidymodels.
See [list of tidymodels](https://www.tidymodels.org/find/parsnip/) for a complete list of the 
supported regression models (currently 80).



Note that we use the same data and predictive models in this vignette as in the main vignette.


# The separate regression method class {#separate}
In the \separatereg\ methods, we train a new regression model $g_S(\boldsymbol{x}s)$ to estimate
the conditional expectation for each coalition of features. 

The idea is to estimate $v(S) = E[f(\boldsymbol{x})|\boldsymbol{x}_S = \boldsymbol{x}_S^*] = E[f(\boldsymbol{x}_{\bar{S}},\boldsymbol{x}_S)|\boldsymbol{x}_S=\boldsymbol{x}_S^*]$ separately
for each coalition $S$ using regression. Let $X = \{ \boldsymbol{x}^{[i]}, y^{[i]} \}_{i=1}^{N_{\text{train}}}$
denote the training data, where $\boldsymbol{x}^{[i]}$ is the $i$th $M$-dimensional input and $y^{[i]}$
is the associated response. For each coalition $S \subseteq \{1,2,\dots,M\}$, the corresponding training data set is
\begin{align*}
            X_S 
            =
            \{\boldsymbol{x}_S^{[i]}, f(\underbrace{\boldsymbol{x}_\bar{S}^{[i]}, \boldsymbol{x}_S^{[i]}}_{\boldsymbol{x}^{[i]}})\}_{i=1}^{N_{\text{train}}}
            =
            \{\boldsymbol{x}_S^{[i]}, \underbrace{f(\boldsymbol{x}^{[i]})}_{z^{[i]}}\}_{i=1}^{N_{\text{train}}}
            =
            \{\boldsymbol{x}_S^{[i]}, z^{[i]}\}_{i=1}^{N_{\text{train}}}.
\end{align*}

For each data set $X_S$, we train a regression model $g_S(\boldsymbol{x}s)$ with respect to the mean squared error loss
function. The optimal model, with respect to the loss function, is 
$g^*_S(\boldsymbol{x}_S) = E[z|\boldsymbol{x}_S] = E[f(\boldsymbol{x}_\bar{S}, \boldsymbol{x}_S)|\boldsymbol{x}_S]$,
which corresponds to the contribution function $v(S)$. The regression model $g_S$ aims for the optimal, hence, it
resembles/estimates the contribution function, i.e., 
$g_S(\boldsymbol{x}_S) = \hat{v}(S) \approx v(S) = E[f(\boldsymbol{x}_\bar{S}, \boldsymbol{x}_S) | \boldsymbol{x}_S = \boldsymbol{x}_S^*]$. 



## Code {#separate_code}
In the following examples we use the same data and want to explain the same model as in the main vignette.
That is, we train a simple `xgboost` model on the `airquality` dataset, and illustrate how `shapr` and 
the separate regression method class can be used to explain the individual predictions.

```{r}
library(xgboost)
library(data.table)
library(tidymodels)
library(shapr)

data("airquality")
data <- data.table::as.data.table(airquality)
data <- data[complete.cases(data), ]

x_var <- c("Solar.R", "Wind", "Temp", "Month")
y_var <- "Ozone"

ind_x_explain <- 1:20
x_train <- data[-ind_x_explain, ..x_var]
y_train <- data[-ind_x_explain, get(y_var)]
x_explain <- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model <- xgboost::xgboost(
  data = as.matrix(x_train),
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Specifying the phi_0, i.e. the expected prediction without any features
p0 <- mean(y_train)

# Specify that we are using the regression paradigm and the separate method class
approach <- "regression_separate"

# Specify that we want to use a linear regression model
regression_model <- parsnip::linear_reg()

# Compute the Shapley value explanations
explanation_sep_lm <- shapr::explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  approach = approach,
  regression_model = parsnip::linear_reg()
)
```

A linear model is often not flexible enough and can produce inaccurate Shapley value explanations. 
We can compare with using the `empirical` approach and see see that
the `empirical` approach outperforms the linear regression model approach quite significantly.

```{r}
# Compute the Shapley value explanations using the empirical method
explanation_empirical <- shapr::explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "empirical",
  prediction_zero = p0
)

# Compare the MSEv criterion of the different explanation methods
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_linear_model = explanation_sep_lm
)) +
  theme(legend.position = "none") # Remove the legend
```

In the next example, we use a decision tree model. The `tidymodels` package supports several implementations,
so we have to use `set_engine()` to specify which one to use and we need to specify that we are going to do 
regression by using `set_mode()`. Note that we do not specify the parameters of the decision tree, which means
that `tidymodels` will use the default parameters set in `rpart`.

By searching for "decision tree" in [list of tidymodels](https://www.tidymodels.org/find/parsnip/), we will see
that the default parameters for [`decision_tree_rpart`](https://parsnip.tidymodels.org//reference/details_decision_tree_rpart.html) are `tree_depth = 30`, `min_n = 2`, and `cost_complexity = 0.01`.

```{r}
# Set the parameters (stumps)
explanation_sep_tree_stump <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 1,
  approach = "regression_separate",
  regression_model = decision_tree(tree_depth = 1, min_n = 2, cost_complexity = 0.01) %>%
    set_engine("rpart") %>%
    set_mode("regression")
)

# Using default parameters
explanation_sep_tree_default <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 1,
  approach = "regression_separate",
  regression_model = decision_tree() %>%
    set_engine("rpart") %>%
    set_mode("regression")
)
```

We can now compare the two new methods. We see that the decision tree with default parameters (177.7) outperform 
the linear model approach (745.2) with respect to the MSEv criterion, and is on the same level as the empirical
approach (179.4). By using stumps, i.e., trees with depth one, we obtained a worse method (218.0).

```{r}
# Compare the MSEv criterion of the different explanation methods
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_linear_model = explanation_sep_lm,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default
)) +
  theme(legend.position = "none") # Remove the legend
```

### Cross validation

Another option is to use cross validation to tune the hyperparameters. To do this, we need to specify three things:

1. In `regression_model`, we need to specify which parameters to tune in the model. This is done by setting the parameter
equal to `tune()`. E.g., if we want to tune the `tree_depth` in the `decision_tree`,
while using default parameters for the other parameters, then we set `decision_tree(tree_depth = tune())`.
2. In `regression_tune_values`, we must provide a data.frame/data.table/tibble of the possible hyperparameter values
to consider. The column names of `regression_tune_values` must match the tuneable hyperparameters specified in
`regression_model`. I.e., for the example above, `regression_tune_values` must be a one column data.frame with the name
`tree_depth`. This can either be done by specifying them manually, or use the `dials` package, e.g.,
`dials::grid_regular(dials::tree_depth(), levels = 5)`.
3. It is optional to specify the `regression_vfold_cv_para` parameter. If used, then `regression_vfold_cv_para` must
be a named list specifying the parameters to send to the cross-validation function `rsample::vfold_cv()`. 
See `?vfold_cv` to see the default parameters. The names in `regression_vfold_cv_para` must match the parameter names in
`rsample::vfold_cv()`. For example, if we want 5-fold cv, then we set `regression_vfold_cv_para = list(v = 5)`.


Lets look at some ways to specify `regression_tune_values`.
```{r echo=TRUE, results='hide'}
# Possible ways to define the `regression_tune_values` object. One automatic and three manual ways.
dials::grid_regular(dials::tree_depth(), levels = 4)
data.table(tree_depth = c(1, 5, 10, 15)) # Note that we can also use data.frame and tibble too.

# For several features
dials::grid_regular(dials::tree_depth(), dials::cost_complexity(), levels = 3)
expand.grid(tree_depth = c(1, 3, 5), cost_complexity = c(0.001, 0.05, 0.01))
```


Then we use cross validation on the `tree_depth` parameter, and we see that we obtain better MSEv results (169.96).
```{r}
# Use trees with cross validation on the depth
explanation_sep_tree_cv <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  verbose = 2,
  approach = "regression_separate",
  regression_model = decision_tree(tree_depth = tune()) %>%
    set_engine("rpart") %>%
    set_mode("regression"),
  regression_tune_values = dials::grid_regular(dials::tree_depth(), levels = 4),
  regression_vfold_cv_para = list(v = 5)
)

# Compare the MSEv criterion of the different explanation methods. Remove linear_model
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv
)) +
  theme(legend.position = "none") # Remove the legend
```
```{r}
# Use trees with cross validation on the depth and cost complexity. Manually set the values
explanation_sep_tree_cv_2 <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  verbose = 2, # If we want a prinout of the results
  approach = "regression_separate",
  regression_model = decision_tree(tree_depth = tune(), cost_complexity = tune()) %>%
    set_engine("rpart") %>%
    set_mode("regression"),
  regression_tune_values = expand.grid(tree_depth = c(1, 3, 5), cost_complexity = c(0.001, 0.01, 0.1)),
  regression_vfold_cv_para = list(v = 5)
)

# Compare the MSEv criterion of the different explanation methods. Remove linear_model
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv,
  sep_tree_cv_2 = explanation_sep_tree_cv_2
)) +
  theme(legend.position = "none") # Remove the legend
```


Note that this a bit of an unfair comparison for the `empirical` approach which also has hyper-parameters one could
tune. However, `shapr` does not currently provide a function to do this automatically.


Furthermore, we have to consider that doing the cross validation drastically increase the elapsed time in seconds.
```{r}
# Elapsed time for the different methods
unlist(sapply(list(
  empirical = explanation_empirical,
  sep_linear_model = explanation_sep_lm,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv,
  sep_tree_cv_2 = explanation_sep_tree_cv_2
), "[[", "timing")[2, ])
```



```{r}
explanation_sep_xgboost <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  verbose = 2, # If we want a prinout of the results
  approach = "regression_separate",
  regression_model = boost_tree() %>% set_engine("xgboost") %>% set_mode("regression")
)


library(progressr)
library(future)
future::plan(multisession, workers = 4)
progressr::handlers("cli") # requires the 'cli' package
progressr::with_progress({
  explanation_sep_xgboost_cv <- explain(
    model = model,
    x_explain = x_explain,
    x_train = x_train,
    prediction_zero = p0,
    n_batches = 15,
    verbose = 2, # If we want a printout of the results
    approach = "regression_separate",
    regression_model = boost_tree(trees = tune()) %>%
      set_engine("xgboost") %>%
      set_mode("regression"),
    regression_tune_values = dials::grid_regular(dials::tree_depth(),
      dials::trees(range = c(50, 1000)),
      dials::learn_rate(),
      levels = 4
    ),
    regression_vfold_cv_para = list(v = 4)
  )
})
future::plan(sequential) # To return to non-parallel computation

# Compare the MSEv criterion of the different explanation methods. Remove linear_model
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv,
  sep_tree_cv_2 = explanation_sep_tree_cv_2,
  sep_xgboost = explanation_sep_xgboost,
  sep_xgboost_cv = explanation_sep_xgboost_cv
)) +
  theme(legend.position = "none") # Remove the legend
```



# The surrogate regression method class {#surrogate}


Since the \separatereg\ methods train a new regression model $g_S(\boldsymbol{x}_S)$ for each coalition
$S \subseteq \{1,2,\dots,M\}$, a total of $2^M-2$ models has to be trained, which can be time-consuming for
slowly fitted models. The minus two corresponds to the empty and grand coalitions.
The \surrogatereg\ method class builds on the ideas from the \separatereg\ class, but instead of fitting a
new regression model for each coalition, we train a single regression model $g(\tilde{\boldsymbol{x}}_S)$
for all coalitions $S \subseteq \{1,2,\dots,M\}$ (except empty and grand coalitions),
where $\tilde{\boldsymbol{x}}_S$ is an augmented version of $\boldsymbol{x}_S$. See Section 3.6.1 in
@olsen2023comparative for more details and examples.



## Code {#surrogate_code}

```{r}
# Specify that we are using the regression paradigm and the separate method class
approach <- "regression_surrogate"

# Specify that we want to use a linear regression model
regression_model <- parsnip::linear_reg()

# Compute the Shapley value explanations
explanation_sur_lm <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = approach,
  verbose = 2,
  regression_model = parsnip::linear_reg()
)

# Compare the MSEv criterion of the different explanation methods. Remove linear_model
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv,
  sep_tree_cv_2 = explanation_sep_tree_cv_2,
  sep_lm = explanation_sep_lm,
  sur_lm = explanation_sur_lm
)) +
  theme(legend.position = "none") # Remove the legend
```
```{r}
# Using random forest with default parameters as the surrogate model
explanation_sur_rf <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = approach,
  verbose = 2,
  regression_model = rand_forest() %>%
    set_engine("ranger") %>%
    set_mode("regression")
)

# Using random forest with parameters tuned by cv as the surrogate model
explanation_sur_rf <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = approach,
  verbose = 2,
  regression_model = rand_forest(mtry = tune(), trees = tune()) %>%
    set_engine("ranger") %>%
    set_mode("regression"),
  regression_tune_values =
    dials::grid_regular(dials::mtry(c(1, ncol(x_explain) - 1)), dials::trees(c(10, 750)), levels = 4),
  regression_vfold_cv_para = list(v = 5)
)

# Using xgboost with default parameters as the surrogate model
explanation_sur_xgboost <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  n_batches = 4,
  approach = approach,
  verbose = 2,
  regression_model = boost_tree() %>%
    set_engine("xgboost") %>%
    set_mode("regression")
)

# Compare the MSEv criterion of the different explanation methods. Remove linear_model
plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv,
  sep_tree_cv_2 = explanation_sep_tree_cv_2,
  sur_rf = explanation_sur_rf,
  sur_rf_cv = explanation_sur_rf_cv,
  sur_xgboost = explanation_sur_xgboost
)) +
  theme(legend.position = "none") # Remove the legend
```

```{r}
plot_SV_several_approaches(list(
  empirical = explanation_empirical,
  sep_tree_stump = explanation_sep_tree_stump,
  sep_tree_default = explanation_sep_tree_default,
  sep_tree_cv = explanation_sep_tree_cv,
  sep_tree_cv_2 = explanation_sep_tree_cv_2,
  sur_rf = explanation_sur_rf,
  sur_rf_cv = explanation_sur_rf_cv,
  sur_xgboost = explanation_sur_xgboost
), index_explicands = 1:4)
```



# Add new regression methods {#new}

Even though the `tidymodels` package contain a lot of models (See [list of tidymodels](https://www.tidymodels.org/find/parsnip/)), we might want to add additional methods.
In the following section, we illustrate how to add a the projection pursuit regression (PPR) model as 
new method that can be used by `shapr` to compute the Shapley value explanations; both as a separate and surrogate method.

We use the `ppr()` implementation in the `stats` package to fit the PPR model. The model has several
hyperparameters that can be tuned, but the main hyperparameter is the number of terms `nterms`.
The following is based on [tidymodels' guide](https://www.tidymodels.org/learn/develop/models/) and
we refer to that guide for more details and explanations of the code below.


```{r}
library(parsnip)

  # Step 1: register the model, modes, and arguments
  parsnip::set_new_model(model = "ppr_reg")
  parsnip::set_model_mode(model = "ppr_reg", mode = "regression")
  parsnip::set_model_engine(model = "ppr_reg", mode = "regression", eng = "ppr")
  parsnip::set_dependency("ppr_reg", eng = "ppr", pkg = "stats")

  # If your function has several parameters, then we add one of these functions for each parameter
  parsnip::set_model_arg(
    model = "ppr_reg",
    eng = "ppr",
    original = "nterms", # The original parameter name used in stats::ppr
    parsnip = "num_terms", # Change parameter name to match tidymodels' name convention
    func = list(pkg = "dials", fun = "num_terms"), #list(pkg = "stats", fun = "ppr"),
    has_submodel = FALSE
  )

  # Step 2: create the model function
  ppr_reg <- function(mode = "regression", num_terms = NULL) {
    # Check for correct mode
    if (mode  != "regression") rlang::abort("`mode` should be 'regression'")

    # Capture the arguments in quosures
    args <- list(num_terms = rlang::enquo(num_terms))

    # Save some empty slots for future parts of the specification
    parsnip::new_model_spec(
      "ppr_reg",
      args = args,
      eng_args = NULL,
      mode = mode,
      method = NULL,
      engine = NULL
    )
  }

  # Step 3: add a fit module
  parsnip::set_fit(
    model = "ppr_reg",
    eng = "ppr",
    mode = "regression",
    value = list(
      interface = "formula",
      protect = c("formula", "data", "weights"),
      func = c(pkg = "stats", fun = "ppr"),
      defaults = list()
    )
  )

  parsnip::set_encoding(
    model = "ppr_reg",
    eng = "ppr",
    mode = "regression",
    options = list(
      predictor_indicators = "traditional",
      compute_intercept = TRUE,
      remove_intercept = TRUE,
      allow_sparse_x = FALSE
    )
  )

  # Step 4: add modules for prediction
  parsnip::set_pred(
    model = "ppr_reg",
    eng = "ppr",
    mode = "regression",
    type = "numeric",
    value = list(
      pre = NULL,
      post = NULL,
      func = c(fun = "predict"),
      args = list(
        object = quote(object$fit),
        newdata = quote(new_data),
        type = "numeric"
      )
    )
  )

  # Step 5: add tuning function (used by tune::tune_grid())
  tunable.ppr_reg <- function(x, ...) {
    tibble::tibble(
      name = c("num_terms"),
      call_info = list(list(pkg = NULL, fun = "num_terms")),
      source = "model_spec",
      component = "ppr_reg",
      component_id = "main"
    )
  }

  # Step 6: add updating function (used by tune::finalize_workflow())
  update.ppr_reg <- function(object, parameters = NULL, num_terms = NULL, ...) {
    rlang::check_installed("parsnip")
    eng_args <- parsnip::update_engine_parameters(object$eng_args, fresh = TRUE, ...)
    args <- list(num_terms = rlang::enquo(num_terms))
    args <- parsnip::update_main_parameters(args, parameters)
    parsnip::new_model_spec(
      "ppr_reg",
      args = args,
      eng_args = eng_args,
      mode = object$mode,
      method = NULL,
      engine = object$engine
    )
  }
```
We can now test it:

```{r}

data("airquality")
data <- data.table::as.data.table(airquality)
data <- data[complete.cases(data), ]

x_var <- c("Solar.R", "Wind", "Temp", "Month")
y_var <- "Ozone"

ind_x_explain <- 1:20
x_train <- data[-ind_x_explain, ..x_var]
y_train <- data[-ind_x_explain, get(y_var)]
x_explain <- data[ind_x_explain, ..x_var]

# Fitting a basic xgboost model to the training data
model <- xgboost::xgboost(
  data = as.matrix(x_train),
  label = y_train,
  nround = 20,
  verbose = FALSE
)

# Specifying the phi_0, i.e. the expected prediction without any features
p0 <- mean(y_train)

# Specify that we are using the regression paradigm and the separate method class
approach <- "regression_separate"

# Compute the Shapley value explanations
explanation_sep_ppr <- shapr::explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  approach = approach,
  regression_model = ppr_reg(num_terms = 2) %>% set_engine("ppr") %>% set_mode("regression")
)

# Compute the Shapley value explanations
explanation_sep_ppr_cv <- shapr::explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  prediction_zero = p0,
  approach = approach,
  regression_model = 
    ppr_reg(num_terms = tune()) %>% set_engine("ppr") %>% set_mode("regression"),
  regression_tune_values = dials::grid_regular(dials::num_terms(c(1, 4)), levels = 3),
  regression_vfold_cv_para = list(v = 10)
)

# Compare the MSEv criterion of the different explanation methods
shapr::plot_MSEv_eval_crit(list(
  empirical = explanation_empirical,
  sep_linear_model = explanation_sep_lm,
  sep_ppr_model = explanation_sep_ppr,
  sep_ppr_model_cv = explanation_sep_ppr_cv
)) +
  theme(legend.position = "none") # Remove the legend
```

