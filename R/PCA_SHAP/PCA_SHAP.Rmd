---
title: "Shapley Values & Principal Component Analysis"
author: "Lars H. B. Olsen"
date: '`r Sys.Date()`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Idea

In this file we look at the idea about using Principal Component Analysis to speed up the computations of the
Shapley value computations. 

## The traditional setup
1. We are given a model $f(x)$ and the corresponding training set $\{X_\text{train},\, y_\text{train}\}$. 
    + We assume that there are some dependency structure between the $M$ features. For example, let us assume that 
    $x \sim \mathcal{N}_M(\mu, \Sigma)$, where we for the moment let $\mu = \mathbf{0}$ and $\Sigma$ being $1$ on the
    diagonal and $\rho = 0.7$ off-diagonal.
2. We are asked to explain the prediction of $f(x^*)$ for a new observation/explicand $x^*$.
3. Either we can compute the conditional Shapley value explanations $\boldsymbol{\phi}$ using the `shapr` library 
where we specify that the `approach = "gaussian"`, meaning that we assume that data follows a multivariate Gaussian
distributions. Denote the corresponding Shapley values for $\phi_{\boldsymbol{x}}$. 
Conditional Shapley values can be/are computationally expensive to compute.

## The new setup
1. The new idea is to use principal component analysis to transform the features $\boldsymbol{x}$ into independent
features $\boldsymbol{z}$ and then use marginal Shapley values, i.e., `approach = "independence"` in `shapr`.
Marginal Shapley values are faster to compute as they create the Monte Carlo samples by sampling observations 
from th training data.
    + That is, we have a the principal component function $h: \boldsymbol{x} \mapsto \boldsymbol{z}$, where
    $\boldsymbol{z}$ are the principal components and $h$ is a linear function. Linear in the sense that 
    $\boldsymbol{z} = h(\boldsymbol{x}) = \boldsymbol{A} \boldsymbol{x}$. Here $\boldsymbol{A}$ is a square matrix of
    dimension $M \times M$ and is the rotation matrix (note that `scale` and `center` will influence the PCA
    transformation). The function $h$ also has an inverse $h^{-1}: \boldsymbol{z} \mapsto \boldsymbol{x}$.
2. In the first place, we will use all $M$ principal components. Later we can try to reduce it.
3. Instead of letting the contribution/reward function be
$v(\mathcal{S}) = \mathbb{E}[f(\boldsymbol{x}) | \boldsymbol{x}_{\mathcal{S}}] = \mathbb{E}[f(\{\boldsymbol{x}_{\bar{\mathcal{S}}}, \boldsymbol{x}_{\mathcal{S}}\}) | \boldsymbol{x}_{\mathcal{S}}]$, 
we compute $\tilde{v}(\mathcal{S}) = \mathbb{E}[f(h^{-1}(\boldsymbol{z})) | \boldsymbol{z}_{\mathcal{S}}] = \mathbb{E}[f(h^{-1}(\{\boldsymbol{z}_{\bar{\mathcal{S}}}, \boldsymbol{z}_{\mathcal{S}}\})) | \boldsymbol{z}_{\mathcal{S}}]= \mathbb{E}[f(\{\boldsymbol{x}_{\bar{\mathcal{S}}}, \boldsymbol{x}_{\mathcal{S}}\}) | \boldsymbol{z}_{\mathcal{S}}]$.
4. Then we compute the corresponding Shapley values $\phi_{\boldsymbol{z}}$ for the $\boldsymbol{z}$ features/principal components.
5. The idea is then to transform $\phi_{\boldsymbol{z}}$ into $\phi_{\boldsymbol{x}}$ using $h^{-1}$.
    + I think the problem is here. Why should the transformations on the feature space also be
    valid for the Shapley value space.
    + Indirectly, it think we are saying that $\phi_{\boldsymbol{z}}$ are independent and when we apply the $h^{-1}$, we
    reintroduce the dependence structure that $\boldsymbol{x}$ had (?)
 
## Conclusion
I might have done something wrong in the code or misunderstood some ideas. 
But I do not get the same Shapley values with the two procedures.
I am open for setting up a meeting and discussing this more.

I am unsure why using $h^{-1}$, which is $h^{-1}: \boldsymbol{z} \mapsto \boldsymbol{x}$, should also work on 
$\phi_{\boldsymbol{z}} \mapsto \phi_{\boldsymbol{x}}$. I feel like these are two different spaces.

Also, due to the efficiency axiom, we have that $\sum_{j=1}^M \phi_{\boldsymbol{z},j}^* = f(x^*)$.
But when we are sending $\phi_{\boldsymbol{z}}$ through $h^{-1}$ to get the Shapley values in the original feature space 
we do NOT have that the efficiency axiom still holds.

Is there any way to ensure that it still holds?

Maybe if we assume that $f$ is linear that we are able to derive some results. Since both the PC transformation and 
the model is linear.







# Code

Load the needed libraries. The github branch is a branch where I have altered the `shapr` code to transform the 
principal components back to original feature space before sending the Monte Carlo samples to the model.
```{r setup_libraries, results = 'hide'}
library(MASS)
library(data.table)
library(progressr)
suppressMessages(remotes::install_github("LHBO/shapr", ref = "Lars/PCA_SHAP"))
library(shapr)
# devtools::load_all(".")
```


We first set the parameters of our simulation experiment.
```{r setup_parameters}
# Set seed for reproducibility
set.seed(123)

# Parameters
n_train <- 10000 # Number of training samples
n_explain <- 1000 # Number of explicands
n_features <- 3 # Number of features

# Mean and covariance matrix for the multivariate Gaussian distribution
mean_vector <- rep(0, n_features)
rho <- 0.75
cov_matrix <- matrix(rho, nrow = n_features, ncol = n_features)
diag(cov_matrix) <- 1

# Another option
# cov_matrix <-
#   matrix(c(1, 0.5, 0.75, 0.5, 1, 0.2, 0.75, 0.2, 1), nrow = n_features, byrow = TRUE)
```
The we create the multivariate Gaussian distributed features. Both the training and explain data sets.

```{r setup_generate_data}
# Generate x_train from multivariate Gaussian
x_train <- MASS::mvrnorm(n = n_train, mu = mean_vector, Sigma = cov_matrix)

# Generate x_explain from multivariate Gaussian with
# the same mean and covariance matrix as x_train
x_explain <- MASS::mvrnorm(n = n_explain, mu = mean_vector, Sigma = cov_matrix)

# Set column names
colnames(x_train) <- colnames(x_explain) <- paste0("X", seq(n_features))

# Display x_train and x_explain
pairs(x_train)
pairs(x_explain)
```


Time to create the response and the model we want to explain. We let the response be a linear combination of the
features and we let the model we a simple linear regression model.
```{r create_model}
# Create a very simple linear response. To keep it simple, we do not add any noise now.
beta <- c(3, -2, 1)
y_train <- x_train %*% beta
y_explain <- x_explain %*% beta

# Make data tables out of the training and test features + response
data_train <- as.data.table(cbind(y_train, x_train))
data_explain <- as.data.table(cbind(y_explain, x_explain))
colnames(data_train) <- colnames(data_explain) <- c("y", paste0("X", seq(n_features)))

# Train a linear model
model <- lm(y ~ ., data = data_train)
```


## Conditional Shapley values
We compute the conditional Shapley values using the `shapr` package where we use the `gaussian` approach that assumes
that the data follows a multivariate Gaussian distribution.

```{r explain_CSV}
# # Set up progressr, so we get feedback from shapr
# progressr::handlers(global = TRUE)
# progressr::handlers('cli') # requires the 'cli' package

# Explain the model using conditional shapley values
explanation <- explain(
  model = model,
  x_explain = x_explain,
  x_train = x_train,
  approach = "gaussian",
  n_batches = 5,
  prediction_zero = 0, # Set \phi_0 to be 0 as we only want $M$ dimensional Shapley values
  n_samples = 2500 # The number of Monte Carlo samples to use
)

plot(explanation, index_x_explain = 1:6)
```





## Principal components

We now start to look at the PCA + marginal Shapley value idea.

We start by computing the PC for the training data. We see that the PCs are independent.

```{r pca_train}
# Compute principal components for x_train
pca_train <- prcomp(x_train, center = TRUE, scale. = TRUE)
x_train_pca <- pca_train$x

# Look at some summary to see the importance of each principal component
summary(pca_train)

# Compute PC means and covariance
colMeans(x_train_pca) # Technically zero
cov(x_train_pca)
cor(x_train_pca)

# Can also look at the pairs plot
pairs(x_train)
pairs(x_train_pca)
```

Then it is time to apply the same transformation on the explicands.

```{r pca_explain}
# Use the same transformations on x_explain
x_explain_pca <- predict(pca_train, newdata = x_explain)
# Now, x_explain_pca contains the principal components of x_explain
# based on the transformations learned from x_train

# x_explain_tilde = t(t(x_explain_pca %*% t(pca_rotation)) * pca_scale + pca_center)
# x_explain[1:10,]
# x_explain_tilde[1:10,]

# Compute PC means and covariance
# The means should be close to zero, but not exactly as we
# use the training data transformations
colMeans(x_explain_pca)
cov(x_explain_pca)
cor(x_explain_pca)

# Can also look at the pairs plot
pairs(x_explain_pca)
```


To go from feature space to principal component space I use the `predict` function as above. 
This constitutes the $h: \boldsymbol{x} \mapsto \boldsymbol{z}$ function discussed above.

To go from PC space back to feature space, i.e., $h^{-1}: \boldsymbol{z} \mapsto \boldsymbol{x}$, I use the
following computations:

```{r pca_check}
# If we are to center or scale the data.
# REMEMBER TO USE THE SAME AS ABOVE TRUE AND TRUE
center <- TRUE
scale. <- TRUE

# Apply the h(x) function on x to get to z
pca_train <- prcomp(x_train, center = center, scale. = scale.)

# Extract the rotation matrix and if we centered or not
pca_rotation <- pca_train$rotation
pca_center <- if (any(pca_train$center == FALSE)) rep(0, ncol(pca_train$x)) else pca_train$center
pca_scale <- if (any(pca_train$scale == FALSE)) rep(1, ncol(pca_train$x)) else pca_train$scale

# Apply the h^{-1}(z) function on z to get to x
h_inverse_of_z <- t(t(pca_train$x %*% t(pca_rotation)) * pca_scale + pca_center)

# This yields equal results for all combinations of `center` and `scale.`
all.equal(x_train, h_inverse_of_z)
```

We can then compute the marginal Shapley values using `shapr` by specifying that we want to use the `independence`
approach. The input to the `shapr` function is now the model $f(\boldsymbol{x})$, the principal component representation
of training data, and the principal component representation of explicands.
Furthermore, we provide the `pca_rotation`, `pca_center` and `pca_scale`.

In this github branch, `shapr` will recognize that the latter three parameters are provided and will then
transform the Monte Carlo samples of the principal components before sending them to the model $f$ using the 
inverse transformation showed above in the previous code block. 


First we choose a low value of `n_samples` as I want to make some pair plots to illustrate that we transform the Monte Carlo principal components. We can chose which combination/coalition we want to look at.
```{r S}
explanation$inter$objects$S
```
That is, `id_combination = 2` means that we just condition on the first feature. Meaning that each row indicates the features that are included in the coalition $\mathcal{S}$. So in `id_combination = 5`, only the third feature is unknown.

For `id_combination = 2`, we see `plot_ggpairs_n_explicands = 10` "lines" in X1, while the other two features are random.
```{r PCA_SHAP_pairs_2}
# HACK NOW. SHAPR needs column names to match, so we rename PC1 -> X1, PC2 -> X2 and so on.
# NEED TO FIX THIS IN A FINAL VERSION IF PCA_SHAP WORKS.
colnames(x_train_pca) <- colnames(x_explain_pca) <- colnames(x_train)

# Create the explanations
explanation_PCA <- explain(
  model = model,
  x_explain = x_explain_pca,
  x_train = x_train_pca,
  approach = "independence", # As we are doing marginal Shapley values
  prediction_zero = 0,
  n_batches = 1, # One batch as we want to plot the data
  n_samples = 50,
  pca_rotation = pca_rotation,
  pca_scale = pca_scale,
  pca_center = pca_center,
  plot_ggpairs = TRUE,
  plot_ggpairs_n_explicands = 10,
  plot_ggpairs_id_combination = c(2),
  keep_samp_for_vS = TRUE
)
```

For `id_combination = 5`, we see `plot_ggpairs_n_explicands = 5` "lines" in X1 and X2, while the last feature is random.
```{r PCA_SHAP_pairs_5}
explanation_PCA <- explain(
  model = model,
  x_explain = x_explain_pca,
  x_train = x_train_pca,
  approach = "independence", # As we are doing marginal Shapley values
  prediction_zero = 0,
  n_batches = 1, # One batch as we want to plot the data
  n_samples = 50,
  pca_rotation = pca_rotation,
  pca_scale = pca_scale,
  pca_center = pca_center,
  plot_ggpairs = TRUE,
  plot_ggpairs_n_explicands = 10,
  plot_ggpairs_id_combination = c(5),
  keep_samp_for_vS = TRUE
)
```

Can also look at all coalitions. Then it is easier to see that we go from something that is independent to having the same 
form as our original features.
```{r PCA_SHAP_pairs_all}
explanation_PCA <- explain(
  model = model,
  x_explain = x_explain_pca,
  x_train = x_train_pca,
  approach = "independence", # As we are doing marginal Shapley values
  prediction_zero = 0,
  n_batches = 1, # One batch as we want to plot the data
  n_samples = 50,
  pca_rotation = pca_rotation,
  pca_scale = pca_scale,
  pca_center = pca_center,
  plot_ggpairs = TRUE,
  plot_ggpairs_n_explicands = 10,
  plot_ggpairs_id_combination = NULL,
  keep_samp_for_vS = TRUE
)
```


Then we skip the plotting and increase `n_samples`. `pairs` is computationally expensive to call when `n_samples` is high.

```{r PCA_SHAP_pairs_FALSE}
explanation_PCA <- explain(
  model = model,
  x_explain = x_explain_pca,
  x_train = x_train_pca,
  approach = "independence", # As we are doing marginal Shapley values
  prediction_zero = 0,
  n_batches = 1,
  n_samples = 2500,
  pca_rotation = pca_rotation,
  pca_scale = pca_scale,
  pca_center = pca_center,
  plot_ggpairs = FALSE,
  keep_samp_for_vS = TRUE
)
```



We can now look at the Shapley value in the two set ups and compare them.

```{r Shap_res1}
# Phi_x
shapley_regular <- as.matrix(explanation$shapley_values[, -1])

# Phi_z
shapley_pca <- as.matrix(explanation_PCA$shapley_values[, -1])

# Phi_z transformed to \tilde{Phi}_x using the same transformation as above
shapley_pca_to_regular <- t(t(shapley_pca %*% t(pca_rotation)) * pca_scale + pca_center)

# Look at the RMSE between the Shapley values and the largest absolute difference
sqrt(mean((shapley_pca_to_regular - shapley_regular)^2))
max(abs(shapley_pca_to_regular - shapley_regular))
```
We see that we get quite a large difference.


```{r}
# We see that the Shapley values do not sum to the right predicted value.
explanation$pred_explain[1:10]
rowSums(shapley_regular)[1:10]
rowSums(shapley_pca_to_regular)[1:10]

# Can scale it, but obviously still wrong signs and values
tmp <- sweep(shapley_pca_to_regular, 1, explanation$pred_explain / rowSums(shapley_pca_to_regular), "*")
rowSums(tmp)[1:10]
explanation$pred_explain[1:10]

shapley_regular[1, ]
tmp[1, ]

sqrt(mean((tmp - shapley_regular)^2))
max(abs(tmp - shapley_regular))
```


We can also look at some plots.

```{r Shap_res2}
pairs(shapley_regular, main = "Phi_x")
pairs(shapley_pca, main = "Phi_z")
pairs(shapley_pca_to_regular, main = "Phi_z_to_x")
```



Or we can combine all of them.
```{r Shap_res3}
library(ggplot2)
library(GGally)
Shapley_value_results <- lapply(list(shapley_pca, shapley_pca_to_regular, shapley_regular), as.data.table)
names(Shapley_value_results) <- c("Phi_z", "Phi_z_to_x", "Phi_x")
Shapley_value_results <- data.table::rbindlist(Shapley_value_results, idcol = "Shapley_type")
GGally::ggpairs(Shapley_value_results,
  columns = 2:4,
  title = "Comparing the Shapley values",
  ggplot2::aes(color = Shapley_type),
  diag = list(continuous = GGally::wrap("densityDiag", alpha = 0.5)),
  lower = list(continuous = GGally::wrap("points", alpha = 0.2))
)

# And plot only with original and the new idea
GGally::ggpairs(Shapley_value_results[Shapley_type %in% c("Phi_x", "Phi_z_to_x")],
  columns = 2:4,
  title = "Comparing the Shapley values",
  ggplot2::aes(color = Shapley_type),
  diag = list(continuous = GGally::wrap("densityDiag", alpha = 0.5)),
  lower = list(continuous = GGally::wrap("points", alpha = 0.2))
)
```


We can look at and compare the Shapley value explanations for the first `6` explicands.
We get very different explanations. Note that the headers are not correct as only the Shapley values of `explanation_x` 
sum to the predicted value `pred`. This is NOT true for `explanation_z_to_x`.
```{r}
explanation_z_to_x <- explanation
explanation_z_to_x$shapley_values <- cbind(explanation_z_to_x$shapley_values[, "none"], shapley_pca_to_regular)
shapr::plot_SV_several_approaches(
  explanation_list = list(
    explanation_x = explanation,
    # explanation_z = explanation_PCA,
    explanation_z_to_x = explanation_z_to_x
  ),
  do_checks = FALSE, # need to set this to false as plot does not work otherwise when including explanation_z
  index_explicands = c(1:6)
)
```




# TRASH
Just me checking that things give the same results:

```{r trash}
x_explain_tilde <- t(t(x_explain_pca %*% t(pca_rotation)) * pca_scale + pca_center)
t(t(as.matrix(explanation_PCA$internal$data$x_explain)[1:10, ] %*% t(pca_rotation)) * pca_scale + pca_center)
x_explain[1:10, ]
x_explain_tilde[1:10, ]
x_explain_pca[1:10, ]
as.matrix(explanation_PCA$internal$data$x_explain)[1:10, ]

predict(model, as.data.table(x_explain[1:10, ]))
predict(model, explanation$internal$data$x_explain[1:10, ])
predict(model, as.data.table(t(t(x_explain_pca %*% t(pca_rotation)) * pca_scale + pca_center))[1:10, ])
predict(model, as.data.table(t(t(as.matrix(explanation_PCA$internal$data$x_explain) %*% t(pca_rotation)) * pca_scale + pca_center))[1:10, ])
explanation_PCA$pred_explain[1:10]
explanation$pred_explain[1:10]

max(abs(explanation_PCA$pred_explain - explanation$pred_explain))

x_train_tilde <- t(t(x_train_pca %*% t(pca_rotation)) * pca_scale + pca_center)
x_train[1:10, ]
x_train_tilde[1:10, ]

rowSums(shapley_pca)[1:10]
rowSums(shapley_pca_to_regular)[1:10]
```
